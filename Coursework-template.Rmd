---
title: "MATH6166/6173 -- Coursework 1 Answer Sheet"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align='center')
```


## Question 1

### Part (a)

<!-- Write your code for Q1 (a) in the code chunk below: -->



```{r Q1a}
#read csv file
house_data <- read.csv(file = "/Users/zheguan/DDA/R/Coursework/melbourne_house_price.csv",header = TRUE, sep = ",")
#we can use View(house_data) to show the structure of original data set
class(house_data)
#here we apply unique function to avoid duplicate in the uncleaned data set.
more4_rooms <- length(unique(house_data$Address[house_data$Rooms > 4]))

more4_rooms
```

<!-- Report your answer for Q1 (a) below: -->

There are 30 unique properties available with more than 4 bedrooms.


### Part (b)

<!-- Write your code for Q1 (b) in the code chunk below: -->


```{r Q1b}
#convert format
house_data$Suburb <- as.factor(house_data$Suburb)
house_data$Rooms <- as.factor(house_data$Rooms)
house_data$Type <- as.factor(house_data$Type)

num_suburbs <- length(levels(house_data$Suburb))

levels(house_data$Suburb)
num_suburbs
```

<!-- Report your answer for Q1 (b) below: -->

there are 5 different suburbs: "Bentleigh East" "Brunswick"      "Preston"   "Reservoir"      "Richmond" 


### Part (c)

<!-- Write your code for Q1 (c) in the code chunk below: -->


```{r Q1c}
# here we remove the records that missed the value in column Building Area and Building Year.

cleaned_data <- house_data[!is.na(house_data$BuildingArea) & !is.na(house_data$YearBuilt),]

#left 657 records
View(cleaned_data)

#filtered_data for properties builded before 1980
filtered_data_1980 <- cleaned_data[cleaned_data$YearBuilt <= 1980, ]

#median building area
median_area <- median(filtered_data_1980$BuildingArea)

median_area

```

<!-- Report your answer for Q1 (c) below: -->

The median building area for properties built in the year 1980 or before is **117**.


### Part (d)

<!-- Write your code for Q1 (d) in the code chunk below: -->


```{r Q1d}

#we use max function to extract the highest price where properties in Brunswick, and then extract the corresponding record in the data set

brunswick_highest_record <- house_data[house_data$Price == max(house_data[house_data$Suburb=='Brunswick',]$Price),]

#paste suburb and address to get total address
brunswick_t_max_address <- paste(brunswick_highest_record$Address,brunswick_highest_record$Suburb,sep = " ")

brunswick_t_max_address
```

<!-- Report your answer for Q1 (d) below: -->

The address with the most expensive townhouse sold in the Brunswick suburb is **11 Heller St Brunswick**


### Part (e)

<!-- Write your code for Q1 (e) in the code chunk below: -->


```{r Q1e}
#check every record if it has missing value

house_data$FullyObserved <- complete.cases(house_data)

#considering True is regarded as 1 in R, so we can sum the list up and got the corresponding total number.

n_missing_area <- sum(is.na(house_data$BuildingArea))
n_missing_year <- sum(is.na(house_data$YearBuilt))

#check which one have the bigger number of missing value
col_miss <- ifelse(n_missing_area>n_missing_year,"BuildingArea","YearBuilt")
```

<!-- Report your answer for Q1 (e) below: -->

BuildingArea Column has the most number of missing values

### Part (f)


<!-- Write your code for Q1 (f) in the code chunk below: -->

```{r Q1f, message=FALSE, warning=FALSE}
#load ggplot2 library
library(ggplot2)

list_q1f <- c("Bentleigh East", "Preston","Reservoir")
filtered_data_Q1f <- house_data[house_data$Suburb %in% list_q1f,]

ggplot(data = filtered_data_Q1f)+
  geom_boxplot(mapping = aes(x= Suburb, y = Price, fill = Suburb))+
  labs(title = "Side-by-side Boxplot of Price for Three Suburbs",
       x = "Suburb",
       y = "Price")+
  theme_minimal()
```

we can easily find Preston has more outliers.
### Part (g)


<!-- Write your code for Q1 (g) in the code chunk below: -->

```{r Q1g}

#with geom_histogram to plot histogram, and fill hist with Room category.

ggplot()+
  geom_histogram(mapping = aes(x = BuildingArea, colour = Rooms,fill = Rooms),data = house_data[house_data$Rooms == 1,] , alpha = 0.5, binwidth = 10)+
  geom_histogram(mapping = aes(x = BuildingArea, colour = Rooms,fill = Rooms), data = house_data[house_data$Rooms == 4,], alpha = 0.5, binwidth = 10)+
  labs(title = "Building Area Distribution by Room Category",
       x = "Building Area",
       y = "Count")+
  xlim(0,300)+
  theme_minimal()

```

### Part (h)

<!-- Write your code for Q1 (h) in the code chunk below: -->


```{r Q1h}

#calculate mean by rooms with tapply function which focus on diferrent categories
mean_by_rooms <- tapply(house_data$BuildingArea, house_data$Rooms, mean, na.rm = TRUE)
mean_by_rooms
global_mean <- mean(house_data$BuildingArea,na.rm = TRUE)
global_mean

#considering ifelse() function is element-wise in R, we can directly use it:
house_data$BuildingArea <-
  ifelse(is.na(house_data$BuildingArea),  #check if it is na in cell
       ifelse(is.na(mean_by_rooms[as.character(house_data$Rooms)]), # check if mean_by_rooms value is provided
                    global_mean,  # if not provided, fill with global mean
                    mean_by_rooms[as.character(house_data$Rooms)]), #if provided, fill with calculated number
       house_data$BuildingArea) #if it is not na in original data set, fill with original value

house_data 
```

## Question 2

### Part (a)


<!-- Write your function for Q2 (a) in the code chunk below: -->


```{r Q2a}
#define distance function, here we use the Euclidean distance as default formula.

dist_func <- function(a, b){

  return( sqrt(sum((a-b)**2) ))

  }

#define classify_point function which is used to judge the category of a new observation x

classify_point <- function(x, training_data, k, dist_func){
  
  #check if k is smaller than the number of training_data

  if (k >= ncol(training_data)){stop("k must smaller than the number of training_data")}

  #we can apply pipe operator |> ( which is supported by R after version 4.1) to increase readibility
  
 x_training_index_bydistance <- training_data[-nrow(training_data),] |> #remove the last result line 
       apply(2, function(col) dist_func(col,x)) |> #apply every column to dis_func
          order()
 
  train_head_k_result <- training_data[nrow(training_data),x_training_index_bydistance[1:k]]  #select the head k columns
                       
  #summarize the number of different categories
  sum_ones <- sum(train_head_k_result == 1)
  sum_twos <- sum(train_head_k_result == 2)
  
  predicted_class <- ifelse( sum_ones >= sum_twos , 
                             # if the number of category 1 bigger than/equal to the number of category 2
                            ifelse (sum_ones == sum_twos, # if two numbers are same
                                    training_data[nrow(training_data),x_training_index_bydistance[k+1]], #judged by k+1 category
                                    1), # return category 1
                            2) #return category 2
  
  return(predicted_class)
}

```


### Part (b)


<!-- Write your function for Q2 (b) in the code chunk below: -->


```{r Q2b}

#Since we have prepared the module for a sparate obeservation, we can directly apply new data matrix with the function
classify_data <- function(test_data, training_data, k, dist_func){

  if(nrow(test_data) != nrow(training_data) - 1) {stop("test_data must have same dimensions with training data!")}
  
  predicted_classes <- test_data |> apply(2,function(col) classify_point(col, training_data, k, dist_func))
  
}

```

### Part (c)


<!-- Write your code for Q2 (c) in the code chunk below: -->


```{r Q2c}

ion_train <- read.table(file = "./ion_train.txt") |> as.matrix()

ion_test <- read.table(file = "./ion_test.txt") |> as.matrix()

predicted_classes <- seq(1,13,1) |> sapply(function(k) classify_data(ion_test,ion_train,k, dist_func)) |> as.data.frame()

predicted_classes
```

### Part (d)


<!-- Write your function for Q2 (d) in the code chunk below: -->


```{r Q2d}

#In fact, there is a "table" command in R which can calculate the confusion matrix automatically, but we can use a loop to calculate by ourselves.

calc_conf_mat <- function(predicted_classes, true_classes){
  #check if the levels are equal in predicted_classes and true_classes
  
  levels_true_classes <- levels(factor(true_classes)) |> as.numeric()
  levels_predicted_classes <- levels(factor(predicted_classes)) |> as.numeric()
  
  if (!setequal(levels_true_classes, levels_predicted_classes)){
    stop("WARNNING: the two numbers of true/predicted categories are not same!!")
  }
  
  #based on the number of categories, set default matrix
  conf_mat <- matrix(data = 0, 
                     nrow = length(levels_true_classes), 
                     ncol = length(levels_predicted_classes),
                     dimnames = list(Actual = levels_true_classes, Predicted = levels_predicted_classes))
  
  #fill matrix elements, here we use level_true to define the fill position of matrix and find corresponding element in levels_classes
  for ( level_true in 1:length(levels_true_classes)){
    for (level_predict in 1:length(levels_predicted_classes)){
      conf_mat[level_true,level_predict] <- sum((predicted_classes == levels_true_classes[level_true]) & (true_classes==levels_predicted_classes[level_predict]))
    }
  }
  
  return(conf_mat)
  
}

```


### Part (e)


<!-- Write your code for Q2 (e) in the code chunk below: -->


```{r Q2e}
View(predicted_classes)
#add the true values for test data
predicted_classes$Result <- c(rep(1,15),rep(2,15))

  
k_values <- seq(1, 13, 1)

error_proportions <- sapply(k_values, function(k) {
  
  predicted <- predicted_classes[,k] 
  true <- predicted_classes$Result
  conf_mat <- calc_conf_mat(predicted, true)
  
  total <- sum(conf_mat)
  correct <- sum(diag(conf_mat))  
  error_rate <- (total - correct) / total
  return(error_rate)
})

error_df <- data.frame(k = k_values, ErrorRate = error_proportions)

print(error_df)
error_df

ggplot(data = error_df)+
  geom_point(mapping = aes(x = k, y = ErrorRate, color = factor(k)))+
  labs(title = "K-values v.s. Error Rate", 
       x = "K value",
       y = "Error Rate")

#we can find some overfitting/underfitting in the plot: Lower values of k can overfit (low bias, high variance), while higher values can underfit (high bias, low variance). 
```



### Part (f)


<!-- Write your code for Q2 (f) in the code chunk below: -->


```{r Q2f}
#define f1, f2, f3
f1 <- function(a, b) {
  return(sum(abs(a - b)))
}

f2 <- function(a, b) {
  return(max(abs(a - b)))
}

f3 <- function(a, b){
  
  return(1 - sum(a * b)/(sqrt(sum(a^2)) * sqrt(sum(b**2))))
  
}

func <- c(f1,f2,f3)

#we only focus on k = 13
k_13 <- 13

#apply f1, f2, f3 to get accuracy separately
accuracy <- sapply(func, function(F) {
  
 predicted_k13 <- classify_data(ion_test,ion_train, k_13, F)
 true_k13 <- c(rep(1,15),rep(2,15))
 conf_mat_k13 <- calc_conf_mat(predicted_k13, true_k13)
 
 total <- sum(conf_mat_k13)
 correct <- sum(diag(conf_mat_k13)) 
 return(correct/total)
})

#get function with biggest number of accuracy
accu_df <- data.frame(fun_name = c("f1", "f2", "f3"), Accuracy = accuracy)
best_func <- accu_df[accu_df$Accuracy == max(accu_df$Accuracy),'fun_name']

```

<!-- Report your answer for Q2 (f) in the code chunk below: -->

The best function based on calculation is ***"f1"***


### Part (g)


<!-- Write your code for Q2 (g) in the code chunk below: -->


```{r Q2g}

#based on previous question, the "f1" has best performance, so we can get the predicted/true labels separately.
predicted_k13 <- classify_data(ion_test,ion_train, k_13, f1)
true_k13 <- c(rep(1,15),rep(2,15))

#prepare data for plotting
plot_data <- data.frame(x = ion_test[6,], y = ion_test[7,], predicted = predicted_k13, actual = true_k13)

ggplot(data = plot_data) +
  geom_point(aes(x = jitter(x, factor = 0.1), 
                 y = jitter(y, factor = 0.1), 
                 colour = factor(actual), 
                 shape = factor(predicted)), 
             size = 2) +
  labs(colour = "Actual Labels", 
       shape = "Pedicted Labels", 
       x = "Dimension 6",  
       y = "Dimension 7") +
  scale_shape_manual(values = c(16, 4)) +  # Customize the shape for predicted categories
  theme_minimal()


```


### Part (h)


<!-- Write your function for Q2 (h) in the code chunk below: -->


```{r Q2h}

classify_data2 <- function(test_data, training_data, k, dist_func){

  #The last row of training_data should only contain numeric values equal to 1 or 2.
  if( sum(training_data[nrow(training_data),] %in% c(1,2)) !=  length(training_data[nrow(training_data),])) {
    stop("Last row of training_data should only contain numeric values equal to 1 or 2.")
  }
  
  #The dimension of observations in test_data and training_data match up, i.e. the number ofrows of test_data should be 1 less than the number of rows of training_data.
  if(nrow(test_data) != nrow(training_data) - 1) {stop("Dimensions of test_data and training_data do not agree.")}
  
  #The argument dist_func should be a function object.
  if( !is.function(dist_func)) {stop("Argument dist_func should be a function object.")}
  
  predicted_classes <- test_data |> apply(2,function(col) classify_point(col, training_data, k, dist_func))
  
}

```



## Question 3


### Part (a)


<!-- Write your code for Q3 (a) in the code chunk below: -->


```{r Q3a}
set.seed(2024) # Do not delete this line of code. Write your answer after this line.

#define parameters
params <- list(theta = 150, n = 500, mu0 = 0, mu1 = 2, variance = 0.8)

sigma <- rnorm(params$n, mean = 0, sd = sqrt(params$variance))


x <- seq(1,500,1) |> sapply(FUN = function(i){
  ifelse( i <= params$theta,
        params$mu0 + sigma[i],
        params$mu1 + sigma[i]
  )
}) |> as.vector()

#colnames(x) <- c('x')


y <- seq(1,500,1) |> sapply(FUN = function(j){
  ifelse( j <= params$theta,
          (1/params$theta) * sum(x[1:params$theta]),
          (1/(params$n - params$theta)) * sum(x[(params$theta+1):params$n]))
    })


df_plot <- data.frame(index = seq(1, 500, 1), x = x, y = y)

ggplot(data = df_plot) +
  geom_line(mapping = aes(color = "Data", x = index, y = x),linewidth = 1)+
  geom_line(mapping = aes(color = "Line Trend",x = index, y = y), linewidth = 1) +  
  geom_vline(aes(xintercept = 150, color = "Change Point"), linetype = "dashed", linewidth = 1) +
  scale_color_manual(values = c("Line Trend" = "#3b5998", "Change Point" = "#ff6f61", "Data" = "#2ca02c")) +  
  labs(title = "Trend Line with Change Point",
       x = "Index",
       y = "Y Value",
       color = "Legend") + 
  theme_minimal()

```


### Part (b)


<!-- Write your function for Q3 (b) in the code chunk below: -->


```{r Q3b}

#To achieve a time complexity of O(n), we can obtain the right_sum in the equation provided in the question by using total_sum - left_sum. At the same time, we can also use the cumsum function to simplify the function and improve its readability.

CUSUM.calc <- function(x){
  
  n <- length(x)
  #left sum in equation
  leftsum_xt <- cumsum(x)
  total_sumx <- leftsum_xt[n]
  
  #right sum in equation can be calculated by total_sumx - leftsum_xt
  rightsum_xt <- rep(total_sumx, n) - leftsum_xt
  
  #then calculate division
  k <- 1:(n-1)
  leftsum_xt <- leftsum_xt[k] / k # we only calculate k from 1 to n-1
  rightsum_xt <- rightsum_xt[k] / (n - k) #here n will automaticaly minus each k element
  Tk <- abs( sqrt(k * (n - k) / n) * (leftsum_xt - rightsum_xt))
  
  return(Tk)
}

CUSUM.calc(x)

```

### Part (c)


<!-- Write your function for Q3 (c) in the code chunk below: -->


```{r Q3c}
noise.es <- function( x ){
  
  n <- length(x)
  
  #calculate Yt
  Yt <- (x[2:n] - x[1:n-1])/2**0.5
  
  median_Yt <- median(Yt)
  
  abs_Yt_median <- abs(Yt - median_Yt)
  
  #scale with 1.48 and calculate sigma.es
  sigma.es <-  1.48 * median(abs_Yt_median)
  
  return(sigma.es)
}

noise.es(x)

```

### Part (d)


<!-- Write your function for Q3 (d) in the code chunk below: -->


```{r Q3d}
cpt.detect <- function(x,   threshold = sqrt(2*log(length(x)))){
  
  x.CUSUM <- CUSUM.calc(x)
  
  sigma.est <- noise.es(x)
  
  cpt <- which.max(x.CUSUM)
  
  is.cpt <- sigma.est**(-1) * x.CUSUM[which.max(x.CUSUM)] >  threshold
  
  x.cpt <- list(x.CUSUM = x.CUSUM, 
                sigma.est = sigma.est, 
                cpt = cpt, 
                is.cpt = is.cpt)
  
  return( x.cpt )
}

cpt.detect(x)

```

### Part (e)


<!-- Write your code for Q3 (e) in the code chunk below: -->


```{r Q3e, eval = TRUE}
# if this code is running slowly and you want to render the file quickly to 
# check the code in other questions, you can temporarily set eval = FALSE in
# the chunk option above. Just remember to change it to eval = TRUE before
# you submit the file!

times <- seq(1,100,1)
n <- c(100, 500, 1000, 2000, 4000)
#define a dataframe to store all related information
duration_df <- data.frame(n = integer(), 
                          iteration = integer(), 
                          elapsed_time = numeric())

for(item in n) {
  for (i in times) {
    y <- rnorm(item, mean = 0, sd = 1)
    tmp <- system.time(
              cpt.detect(y)
            )
   #there are three return variable from system.time() - user.self, sys.self, and elapsed,
   #considering we are only required to get one, here we choose "elapsed time", which indicates the total time
   duration_df <- rbind(duration_df, 
                     data.frame(
                      n = item, 
                      iteration = i, 
                      elapsed_time = tmp[["elapsed"]])
                     )
  }
}


#in fact, R has aggregate function which can achieve similar target with other languages with group_by
average_times <- aggregate(elapsed_time ~ n, data = duration_df, FUN = mean)

ave_run_times <- average_times[,"elapsed_time"]

ggplot(average_times)+
  geom_line(mapping = aes(x = n, y = elapsed_time))+
  geom_point(mapping = aes(x = n, y = elapsed_time)) +
  labs(
    title = "Average Running Times for Different Values of n",
    x = "n (Data Size)",
    y = "Average Elapsed Time (seconds)"
  ) +
  scale_x_continuous(
    breaks = unique(average_times$n), #to ensure that the axis labels match up 
    labels = unique(average_times$n)  #with the five values of n.
  ) +
  theme_minimal()
```

### Part (f) 


<!-- Write your function for Q3 (f) in the code chunk below: -->


```{r Q3f}

perm.test <- function(x, x.CUSUM.max, sigma.est, reps = 199, sig.lvl = 0.05){
  
  #define Ar Vector
  A_r <- c(numeric(0))

  for (i in 1:reps) {
    x_tmp <- x |> sample()
    Tk_i_max <- x_tmp |> CUSUM.calc() |>  max()
    sigma_i <-  x_tmp |> noise.es()
    A_r <- c(A_r, sigma_i**(-1) * Tk_i_max)
  }
  
  #calculate perm.c and p.val
   perm.c <- quantile(A_r, probs = 1 - sig.lvl) |> as.numeric()
   p.val <- sum(A_r >= sigma.est**(-1) * x.CUSUM.max) / length(A_r)
   
   x.perm <- list(perm.c = perm.c, p.val = p.val)
   
   return(x.perm)
   
}

x.perm <- perm.test(x,x |> CUSUM.calc() |> max(), x |> noise.es())

x.perm$perm.c

x.perm$p.val
```


### Part (g)


<!-- Write your function for Q3 (g) in the code chunk below: -->


```{r Q3g}

cpt.detect2 <- function(x, threshold = sqrt(2*log(length(x))), threshold.type, reps = 199, sig.lvl = 0.05){
  
  #calculate threshol/pvalue based on threshold.type
  x.CUSUM <- CUSUM.calc(x)
  sigma.est <- noise.es(x)
  
  if(threshold.type == "perm") {
    
    x.CUSUM.max <- x.CUSUM |> max()
    x.perm <- perm.test(x, x.CUSUM.max, sigma.est, reps, sig.lvl)
    threshold <- x.perm$perm.c
    p.val <- x.perm$p.val
    
  }
  
  cpt <- which.max(x.CUSUM)
  is.cpt <- (sigma.est**(-1) * x.CUSUM[which.max(x.CUSUM)]) >  threshold
  
  x.cpt <- list(
              threshold.type = threshold.type,
              threshold = threshold,
              x.CUSUM = x.CUSUM, 
              sigma.est = sigma.est, 
              cpt = cpt, 
              is.cpt = is.cpt
              )

  if (threshold.type == "perm") {
    x.cpt$p.val <- p.val
  }

  
  return( x.cpt )
}


```

### Part (h)


<!-- Write your code for Q3(h) in the code chunk below: -->


```{r Q3h, fig.height = 8.5, fig.width = 8}

nile.data <- read.table(file = "nile_volume.txt", header = TRUE)

nile.cpt <- cpt.detect2(nile.data$volume,threshold.type = "perm", reps = 499, sig.lvl = 0.01)

n <- nrow(nile.data)

mu_t <- seq(1,n,1) |> sapply(FUN = function(j){
  ifelse( j <= nile.cpt$cpt,
          (1/nile.cpt$cpt) * sum(nile.data$volume[1:nile.cpt$cpt]),
          (1/(n - nile.cpt$cpt)) * sum(nile.data$volume[(nile.cpt$cpt+1):n]))
    })

nileplot1_df <- data.frame(index = seq(1, nrow(nile.data), 1), x = nile.data$volume, y = mu_t)


nileplot2_df <- data.frame(
                    index = seq(1, n - 1, 1),   
                    x = CUSUM.calc(nile.data$volume)/noise.es(nile.data$volume))


#considering that ggarrange() function will need library(ggpubr), so here we use inner R plot functions
par(mfrow = c(2, 1))

# Plot 1
plot(nileplot1_df$index, nileplot1_df$x, type = "l", col = "#2ca02c", lwd = 2,
     main = "Trend Line with Change Point",
     xlab = "Index", ylab = "Y Value")
lines(nileplot1_df$index, nileplot1_df$y, col = "#3b5998", lwd = 2)
abline(v = nile.cpt$cpt, col = "#ff6f61", lty = 2, lwd = 2)
legend("topright", legend = c("Data", "Line Trend", "Change Point"),
       col = c("#2ca02c", "#3b5998", "#ff6f61"), lty = c(1, 1, 2), lwd = 2, bty = "n")

# Plot 2
plot(nileplot2_df$index, nileplot2_df$x, type = "l", col = "#2ca02c", lwd = 2,
     main = "Trend Line with Change Point",
     xlab = "Index", ylab = "T(k) Values")
abline(v = nile.cpt$cpt, col = "#ff6f61", lty = 2, lwd = 2)
abline(h = sqrt(2 * log(nrow(nileplot2_df))), col = "#3b5998", lty = 2, lwd = 2)
abline(h = nile.cpt$threshold, col = "#ffcc00", lty = 2, lwd = 2)
legend("topright", legend = c("Data", "Change Point", "log(2n)", "Threshold"),
       col = c("#2ca02c", "#ff6f61", "#3b5998", "#ffcc00"), lty = c(1, 2, 2, 2), lwd = 2, bty = "n")

```

